{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3v0WuIArhR5mUX8++1ffM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajaranjith/HCL-GenAI-Training/blob/main/Assignment_1_Gold_Badge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K3NU-VZnBI1",
        "outputId": "25056343-7f47-43a6-880b-8817b994c8d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m461.0/461.0 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.9/65.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "#Install dependencies\n",
        "# Install the official Mistral Python SDK\n",
        "!pip install -q mistralai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up API key and client\n",
        "import os\n",
        "import getpass\n",
        "from mistralai import Mistral\n",
        "\n",
        "# Ask for your API key once per Colab session\n",
        "if \"MISTRAL_API_KEY\" not in os.environ or not os.environ[\"MISTRAL_API_KEY\"]:\n",
        "    os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"Enter your Mistral API key: \")\n",
        "\n",
        "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
        "client = Mistral(api_key=api_key)\n",
        "\n",
        "print(\"âœ… Mistral client initialized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF8iKfr9nPE6",
        "outputId": "74516b45-7d2e-4fc5-86d2-25009eaf5957"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your Mistral API key: Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "âœ… Mistral client initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a tiny sentiment dataset and save as JSONL\n",
        "import json\n",
        "\n",
        "# Tiny toy dataset: (text, label)\n",
        "train_examples = [\n",
        "    (\"I love this phone, the battery lasts all day.\", \"positive\"),\n",
        "    (\"Terrible experience. The screen broke in two days.\", \"negative\"),\n",
        "    (\"Amazing camera and very fast performance!\", \"positive\"),\n",
        "    (\"The app keeps crashing, I'm very disappointed.\", \"negative\"),\n",
        "    (\"Works fine but the sound quality could be better.\", \"negative\"),\n",
        "    (\"Best purchase I've made this year.\", \"positive\"),\n",
        "    (\"Really happy with this laptop overall.\", \"positive\"),\n",
        "    (\"Customer support was rude and unhelpful.\", \"negative\"),\n",
        "    (\"This headset is comfortable and sounds great.\", \"positive\"),\n",
        "    (\"The product stopped working after a week.\", \"negative\"),\n",
        "]\n",
        "\n",
        "eval_examples = [\n",
        "    (\"Pretty good overall, just a bit heavy.\", \"positive\"),\n",
        "    (\"Worst customer service I've ever seen.\", \"negative\"),\n",
        "    (\"Battery life is awful and it overheats.\", \"negative\"),\n",
        "    (\"Iâ€™m impressed, it works better than expected.\", \"positive\"),\n",
        "]\n",
        "\n",
        "INSTRUCTION = (\n",
        "    \"You are a sentiment classifier.\\n\"\n",
        "    \"Reply with EXACTLY one word: 'positive' or 'negative'.\\n\"\n",
        "    \"No explanations, no punctuation, just the label.\\n\\n\"\n",
        "    \"Text: \"\n",
        ")\n",
        "\n",
        "def write_jsonl(filename, pairs):\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        for text, label in pairs:\n",
        "            record = {\n",
        "                \"messages\": [\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": INSTRUCTION + text,\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": label,\n",
        "                    },\n",
        "                ]\n",
        "            }\n",
        "            # Each JSON object must be on a single line\n",
        "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "write_jsonl(\"sentiment_train.jsonl\", train_examples)\n",
        "write_jsonl(\"sentiment_eval.jsonl\", eval_examples)\n",
        "\n",
        "print(\"âœ… Wrote sentiment_train.jsonl and sentiment_eval.jsonl\")\n",
        "\n",
        "# Quick sanity check â€“ show first 2 lines\n",
        "!head -n 2 sentiment_train.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCavbMfFnleL",
        "outputId": "09d9f203-9b52-472f-f7e2-0f051d8450f3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Wrote sentiment_train.jsonl and sentiment_eval.jsonl\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"You are a sentiment classifier.\\nReply with EXACTLY one word: 'positive' or 'negative'.\\nNo explanations, no punctuation, just the label.\\n\\nText: I love this phone, the battery lasts all day.\"}, {\"role\": \"assistant\", \"content\": \"positive\"}]}\n",
            "{\"messages\": [{\"role\": \"user\", \"content\": \"You are a sentiment classifier.\\nReply with EXACTLY one word: 'positive' or 'negative'.\\nNo explanations, no punctuation, just the label.\\n\\nText: Terrible experience. The screen broke in two days.\"}, {\"role\": \"assistant\", \"content\": \"negative\"}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload dataset files to Mistral\n",
        "# Upload the training and validation files to Mistral Cloud\n",
        "train_file = client.files.upload(\n",
        "    file={\n",
        "        \"file_name\": \"sentiment_train.jsonl\",\n",
        "        \"content\": open(\"sentiment_train.jsonl\", \"rb\"),\n",
        "    }\n",
        ")\n",
        "\n",
        "eval_file = client.files.upload(\n",
        "    file={\n",
        "        \"file_name\": \"sentiment_eval.jsonl\",\n",
        "        \"content\": open(\"sentiment_eval.jsonl\", \"rb\"),\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"âœ… Uploaded files.\")\n",
        "print(\"Training file id:\", train_file.id)\n",
        "print(\"Eval file id    :\", eval_file.id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBNqbZTynymb",
        "outputId": "8b9cc1fe-fc2a-42fb-e69e-a81566850b10"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Uploaded files.\n",
            "Training file id: 662aa371-3e8d-45dd-9362-466d63444fab\n",
            "Eval file id    : 7da21861-80ee-4f7e-98f0-2f2e650a9417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#(Optional) Pretty-print file metadata\n",
        "import json\n",
        "\n",
        "def pprint_obj(o):\n",
        "    print(json.dumps(o.model_dump(), indent=2))\n",
        "\n",
        "print(\"Training file:\")\n",
        "pprint_obj(train_file)\n",
        "\n",
        "print(\"\\nValidation file:\")\n",
        "pprint_obj(eval_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mSC5F_LoF9U",
        "outputId": "b388ff96-40e7-484f-d4ef-35c8c3f73421"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training file:\n",
            "{\n",
            "  \"id\": \"662aa371-3e8d-45dd-9362-466d63444fab\",\n",
            "  \"object\": \"file\",\n",
            "  \"bytes\": null,\n",
            "  \"created_at\": 1766567464,\n",
            "  \"filename\": \"sentiment_train.jsonl\",\n",
            "  \"purpose\": \"fine-tune\",\n",
            "  \"sample_type\": \"instruct\",\n",
            "  \"source\": \"upload\",\n",
            "  \"num_lines\": 10,\n",
            "  \"mimetype\": \"application/jsonl\",\n",
            "  \"signature\": \"6f8cfef570603e54\"\n",
            "}\n",
            "\n",
            "Validation file:\n",
            "{\n",
            "  \"id\": \"7da21861-80ee-4f7e-98f0-2f2e650a9417\",\n",
            "  \"object\": \"file\",\n",
            "  \"bytes\": null,\n",
            "  \"created_at\": 1766567464,\n",
            "  \"filename\": \"sentiment_eval.jsonl\",\n",
            "  \"purpose\": \"fine-tune\",\n",
            "  \"sample_type\": \"instruct\",\n",
            "  \"source\": \"upload\",\n",
            "  \"num_lines\": 4,\n",
            "  \"mimetype\": \"application/jsonl\",\n",
            "  \"signature\": \"f5f94968f7dd93d0\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a fine-tuning job\n",
        "hyperparams = {\n",
        "    \"training_steps\": 50,   # small for demo\n",
        "    \"learning_rate\": 1e-4,\n",
        "}\n",
        "\n",
        "job = client.fine_tuning.jobs.create(\n",
        "    model=\"open-mistral-7b\",\n",
        "    training_files=[{\"file_id\": train_file.id, \"weight\": 1}],\n",
        "    validation_files=[eval_file.id],\n",
        "    hyperparameters=hyperparams,\n",
        "    auto_start=True,  # start as soon as it validates\n",
        ")\n",
        "\n",
        "print(\"âœ… Created fine-tuning job.\")\n",
        "print(\"Job id:\", job.id)\n",
        "\n",
        "# Optional: inspect full job object\n",
        "import json\n",
        "print(json.dumps(job.model_dump(), indent=2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77khR9e9oJzm",
        "outputId": "48ada19e-c536-4ba3-ad9a-627b23ed7f11"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Created fine-tuning job.\n",
            "Job id: 8cbadd93-1d10-4c16-a1b0-ccc8361878dd\n",
            "{\n",
            "  \"id\": \"8cbadd93-1d10-4c16-a1b0-ccc8361878dd\",\n",
            "  \"auto_start\": true,\n",
            "  \"model\": \"open-mistral-7b\",\n",
            "  \"status\": \"QUEUED\",\n",
            "  \"created_at\": 1766574153,\n",
            "  \"modified_at\": 1766574153,\n",
            "  \"training_files\": [\n",
            "    \"662aa371-3e8d-45dd-9362-466d63444fab\"\n",
            "  ],\n",
            "  \"hyperparameters\": {\n",
            "    \"training_steps\": 50,\n",
            "    \"learning_rate\": 0.0001,\n",
            "    \"weight_decay\": 0.1,\n",
            "    \"warmup_fraction\": 0.05,\n",
            "    \"epochs\": null,\n",
            "    \"seq_len\": 32768,\n",
            "    \"fim_ratio\": null\n",
            "  },\n",
            "  \"validation_files\": [\n",
            "    \"7da21861-80ee-4f7e-98f0-2f2e650a9417\"\n",
            "  ],\n",
            "  \"object\": \"job\",\n",
            "  \"fine_tuned_model\": null,\n",
            "  \"suffix\": null,\n",
            "  \"integrations\": [],\n",
            "  \"trained_tokens\": null,\n",
            "  \"metadata\": {\n",
            "    \"expected_duration_seconds\": null,\n",
            "    \"cost\": 0.0,\n",
            "    \"cost_currency\": null,\n",
            "    \"train_tokens_per_step\": null,\n",
            "    \"train_tokens\": null,\n",
            "    \"data_tokens\": null,\n",
            "    \"estimated_start_time\": null\n",
            "  },\n",
            "  \"job_type\": \"completion\",\n",
            "  \"repositories\": []\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Poll job status until it finishes\n",
        "import time\n",
        "import json\n",
        "\n",
        "def pprint_obj(o):\n",
        "    print(json.dumps(o.model_dump(), indent=2))\n",
        "\n",
        "job_id = job.id\n",
        "\n",
        "retrieved_job = client.fine_tuning.jobs.get(job_id=job_id)\n",
        "print(\"Initial status:\", retrieved_job.status)\n",
        "\n",
        "while retrieved_job.status in (\"QUEUED\", \"VALIDATED\", \"RUNNING\"):\n",
        "    pprint_obj(retrieved_job)\n",
        "    print(f\"Status is {retrieved_job.status} â€“ checking again in 10 seconds...\\n\")\n",
        "    time.sleep(10)\n",
        "    retrieved_job = client.fine_tuning.jobs.get(job_id=job_id)\n",
        "\n",
        "print(\"ğŸš© Final status:\", retrieved_job.status)\n",
        "if retrieved_job.status == \"SUCCEEDED\":\n",
        "    print(\"ğŸ‰ Fine-tuned model:\", retrieved_job.fine_tuned_model)\n",
        "else:\n",
        "    print(\"Job did not succeed â€“ check job details above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8vssmiYoWVf",
        "outputId": "9f8436c1-1979-4a96-f7fd-ce9bf6d8da00"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial status: FAILED_VALIDATION\n",
            "ğŸš© Final status: FAILED_VALIDATION\n",
            "Job did not succeed â€“ check job details above.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the fine-tuned model for inference\n",
        "# Make sure the job actually succeeded\n",
        "if retrieved_job.status != \"SUCCEEDED\":\n",
        "    raise RuntimeError(f\"Job status is {retrieved_job.status}, cannot run inference.\")\n",
        "\n",
        "fine_tuned_model = retrieved_job.fine_tuned_model\n",
        "print(\"Using model:\", fine_tuned_model)\n",
        "\n",
        "test_texts = [\n",
        "    \"This is the best thing I've ever bought.\",\n",
        "    \"Horrible quality, I'm never buying from them again.\",\n",
        "    \"Pretty decent for the price.\",\n",
        "    \"I hate how slow and buggy this is.\",\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    resp = client.chat.complete(\n",
        "        model=fine_tuned_model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": INSTRUCTION + text,\n",
        "            }\n",
        "        ],\n",
        "        # optional: make responses deterministic\n",
        "        temperature=0.0,\n",
        "    )\n",
        "\n",
        "    # Safely extract text from the response via .dict()\n",
        "    resp_data = resp.dict()\n",
        "    content_blocks = resp_data[\"choices\"][0][\"message\"][\"content\"]\n",
        "    # Concatenate all text blocks (there's usually just one)\n",
        "    prediction = \"\".join(block.get(\"text\", \"\") for block in content_blocks).strip()\n",
        "\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(\"Predicted label:\", prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "TDFG3AEUo7Nk",
        "outputId": "c3aa6570-8f5d-46a8-d860-b455e2d49001"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Job status is FAILED_VALIDATION, cannot run inference.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1501592708.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Make sure the job actually succeeded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mretrieved_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"SUCCEEDED\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Job status is {retrieved_job.status}, cannot run inference.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mfine_tuned_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieved_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfine_tuned_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Job status is FAILED_VALIDATION, cannot run inference."
          ]
        }
      ]
    }
  ]
}